{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Credit Pixabay](https://cdn.pixabay.com/photo/2016/04/30/13/12/sutterlin-1362879_1280.jpg)\n",
    "\n",
    "# NLP Sentiment Analysis Handbook <!-- omit in toc -->\n",
    "\n",
    "A Step-By-Step Approach to Understand TextBlob, NLTK, Scikit-Learn, and LSTM  networks \n",
    "applied to Sentiment Analysis.\n",
    "\n",
    "\n",
    "Adaptation, corrections and modifications by Mauro Benetti 03-2021\n",
    "This article is base on :\n",
    "\n",
    "* https://towardsdatascience.com/nlp-sentiment-analysis-for-beginners-e7897f976897\n",
    "* https://www.mdeditor.tw/pl/pISR/zh-hk\n",
    "* https://stackabuse.com/removing-stop-words-from-strings-in-python/\n",
    "\n",
    "For aditonal information please visit:\n",
    "* https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "* http://www.nltk.org/data.html\n",
    "\n",
    "-------------------------------------------------------------------------------------\n",
    "\n",
    "# Introduction\n",
    "\n",
    "**Natural Language Processing** (NLP) is the area of machine learning that focuses on the \n",
    "generation and understanding of language. Its main objective is to enable machines to \n",
    "understand, communicate and interact with humans in a natural way.\n",
    "\n",
    "NLP has many tasks such as Text Generation, Text Classification, Machine Translation, \n",
    "Speech Recognition, Sentiment Analysis, etc. For a beginner to NLP, looking at these \n",
    "tasks and all the techniques involved in handling such tasks can be quite daunting. \n",
    "And in fact, it is very difficult for a newbie to know exactly where and how to start.\n",
    "\n",
    "Out of all the NLP tasks, I personally think that Sentiment Analysis (SA) is probably \n",
    "the easiest, which makes it the most suitable starting point for anyone who wants to \n",
    "start go into NLP.\n",
    "\n",
    "In this article, I compile various techniques of how to perform SA, ranging from simple \n",
    "ones like TextBlob and NLTK to more advanced ones like Sklearn and Long Short Term \n",
    "Memory (LSTM) networks.\n",
    "\n",
    "After reading this, you can expect to understand the followings:\n",
    "\n",
    "*   Toolkits used in SA: TextBlob and NLTK\n",
    "*   Algorithms used in SA: Naive Bayes, SVM, Logistic Regression and LSTM\n",
    "*   Jargons like stop-word removal, stemming, bag of words, corpus, tokenisation etc.\n",
    "*   Create a word cloud\n",
    "\n",
    "The flow of this article:\n",
    "\n",
    "*   Data cleaning and pre-processing\n",
    "*   TextBlob\n",
    "*   Algorithms: Logistic Regression, Naive Bayes, SVM and LSTM\n",
    "    \n",
    "## Problem Formulation\n",
    "\n",
    "In this article, we will work with a data set that consists of 3000 sentences coming from reviews on imdb.com, amazon.com, and yelp.com. Each sentence is labeled according to whether it comes from a positive review (labelled as 1) or negative review (labelled as 0).The folder sentiment_labelled_sentences(containing the data file full_set.txt) should be in the same directory as your notebook/script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file **full_set.txt** should be in the same directory as your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.\\t0\\n',\n",
       " 'Good case, Excellent value.\\t1\\n',\n",
       " 'Great for the jawbone.\\t1\\n',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\\t0\\n',\n",
       " 'The mic is great.\\t1\\n',\n",
       " 'I have to jiggle the plug to get it to line up right to get decent volume.\\t0\\n',\n",
       " 'If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\\t0\\n',\n",
       " 'If you are Razr owner...you must have this!\\t1\\n',\n",
       " 'Needless to say, I wasted my money.\\t0\\n',\n",
       " 'What a waste of money and time!.\\t0\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sentiment_labelled_sentences/full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "content[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove leading and trailing white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [x.strip() for x in content]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the sentences from the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['So there is no way for me to plug it in here in the US unless I go by a converter.',\n",
       " 'Good case, Excellent value.',\n",
       " 'Great for the jawbone.',\n",
       " 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!',\n",
       " 'The mic is great.',\n",
       " 'I have to jiggle the plug to get it to line up right to get decent volume.',\n",
       " 'If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.',\n",
       " 'If you are Razr owner...you must have this!',\n",
       " 'Needless to say, I wasted my money.',\n",
       " 'What a waste of money and time!.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '1', '0', '1', '0', '0', '1', '0', '0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    " \n",
    "Where (-1) represents negative and (1) represents positive \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To input data into the any model, the data input must be in vector form. We will do the \n",
    "following transformations:\n",
    "\n",
    "* Remove punctuation and numbers\n",
    "* Transform all words to lower-case\n",
    "* Remove stop words (e.g. the, a, that, this, it, â€¦)\n",
    "* Tokenizer the texts\n",
    "* Convert the sentences into vectors, using a bag-of-words representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test stop word removal code'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "\n",
    "stoppers = ['a', 'is', 'of','the','this','uhm','uh']\n",
    "\n",
    "removeStopWords(stoppers, \"this is a test of the stop word removal code\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you don't have the list of stopwords for english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(stopwords) #Only run this line the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the stop words with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test stop word removal code.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeStopWords(stops, \"this is a test of the stop word removal code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sentences\n",
    "filtered_words = [word for word in word_list if word not in stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['way plug us unless go converter',\n",
       " 'good case excellent value',\n",
       " 'great jawbone',\n",
       " 'tied charger conversations lasting minutes major problems',\n",
       " 'mic great',\n",
       " 'jiggle plug get line right get decent volume',\n",
       " 'several dozen several hundred contacts imagine fun sending one one',\n",
       " 'razr owner must',\n",
       " 'needless say wasted money',\n",
       " 'waste money time',\n",
       " 'sound quality great',\n",
       " 'impressed going original battery extended battery',\n",
       " 'two seperated mere ft started notice excessive static garbled sound headset',\n",
       " 'good quality though',\n",
       " 'design odd ear clip comfortable',\n",
       " 'highly recommend one blue tooth phone',\n",
       " 'advise everyone fooled',\n",
       " 'far good',\n",
       " 'works great',\n",
       " 'clicks place way makes wonder long mechanism would last']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "### Remove digits \n",
    "digits = [str(x) for x in range(10)]\n",
    "remove_digits = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "# ### Remove punctuation \n",
    "remove_punc = [full_remove(x, list(string.punctuation)) for x in remove_digits]\n",
    "\n",
    "# ### Make everything lower-case and remove any white space ##\n",
    "sents_lower = [x.lower() for x in remove_punc]\n",
    "sents_lower = [x.strip() for x in sents_lower]\n",
    "\n",
    "# ### Remove stop words \n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"English\")\n",
    "\n",
    "def removeStopWords(stopWords, txt):\n",
    "    newtxt = ' '.join([word for word in txt.split() if word not in stopWords])\n",
    "    return newtxt\n",
    "\n",
    "sents_processed = [removeStopWords(stops,x) for x in sents_lower]\n",
    "\n",
    "# ### Results\n",
    "sents_processed[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To remove a list of specific stops from the sentences \n",
    "\n",
    "You can add or remove stop words as per your choice to the existing collection of stop \n",
    "words in NLTK. Before removing or adding stop words in NLTK, let's see the list of all \n",
    "the English stop words supported by NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by converter',\n",
       " 'good case excellent value',\n",
       " 'great for jawbone',\n",
       " 'tied charger for conversations lasting more than minutes major problems',\n",
       " 'mic is great',\n",
       " 'have jiggle plug get line up right get decent volume',\n",
       " 'if you have several dozen or several hundred contacts then imagine fun sending each them one by one',\n",
       " 'if you are razr owner you must have this',\n",
       " 'needless say wasted my money',\n",
       " 'what waste money and time',\n",
       " 'and sound quality is great',\n",
       " 'was very impressed when going original battery extended battery',\n",
       " 'if two were seperated by mere ft started notice excessive static and garbled sound headset',\n",
       " 'very good quality though',\n",
       " 'design is very odd as ear clip is not very comfortable at all',\n",
       " 'highly recommend for any one who has blue tooth phone',\n",
       " 'advise everyone do not be fooled',\n",
       " 'so far so good',\n",
       " 'works great',\n",
       " 'clicks into place in way that makes you wonder how long that mechanism would last']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_set = ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from']\n",
    "sents_processed = [removeStopWords(stop_set,x) for x in sents_lower]\n",
    "sents_processed[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Stop Words to Default NLTK Stop Word List\n",
    "\n",
    "To add a word to NLTK stop words collection, first create an object from the stopwords.\n",
    "words('english') list. Next, use the append() method on the list to add any word to the \n",
    "list.\n",
    "\n",
    "The following script adds the word play to the NLTK stop word collection. Again, we \n",
    "remove all the words from our text variable to see if the word play is removed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'play']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('play')\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words from Default NLTK Stop Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'play']\n"
     ]
    }
   ],
   "source": [
    "all_stopwords.remove('not')\n",
    "print(all_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "It is ok to stop here and move to Tokenization. However, one can continue with stemming. \n",
    "The goal of stemming is too strip off prefixes and suffixes in the word and convert the \n",
    "word into its base form, e.g. studying->study, beautiful->beauty, cared->care, â€¦In NLTK, \n",
    "there are 2 popular stemming techniques called porter and lanscaster.\n",
    "\n",
    "(https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter: ['pleas', \"don't\", 'unbuckl', 'your', 'seat-belt', 'while', 'I', 'am', 'driving,', 'he', 'said']\n",
      "lancaster: ['pleas', \"don't\", 'unbuckl', 'yo', 'seat-belt', 'whil', 'i', 'am', 'driving,', 'he', 'said']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def stem_with_porter(words):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words\n",
    "    \n",
    "def stem_with_lancaster(words):\n",
    "    porter = nltk.LancasterStemmer()\n",
    "    new_words = [porter.stem(w) for w in words]\n",
    "    return new_words    ## Demonstrate ##    \n",
    "str = \"Please don't unbuckle your seat-belt while I am driving, he said\"\n",
    "\n",
    "print(\"porter:\", stem_with_porter(str.split()))\n",
    "\n",
    "print(\"lancaster:\", stem_with_lancaster(str.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Letâ€™s try on our sents_processed to see whether it makes sense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by convert',\n",
       " 'good case excel valu',\n",
       " 'great for jawbon',\n",
       " 'tie charger for convers last more than minut major problem',\n",
       " 'mic is great',\n",
       " 'have jiggl plug get line up right get decent volum',\n",
       " 'if you have sever dozen or sever hundr contact then imagin fun send each them one by one',\n",
       " 'if you are razr owner you must have thi',\n",
       " 'needless say wast my money',\n",
       " 'what wast money and time']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = [stem_with_porter(x.split()) for x in sents_processed]\n",
    "\n",
    "porter = [\" \".join(i) for i in porter]\n",
    "\n",
    "porter[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so ther is no way for me plug in her in us unless go by convert',\n",
       " 'good cas excel valu',\n",
       " 'gre for jawbon',\n",
       " 'tied charg for convers last mor than minut maj problem',\n",
       " 'mic is gre',\n",
       " 'hav jiggl plug get lin up right get dec volum',\n",
       " 'if you hav sev doz or sev hundr contact then imagin fun send each them on by on',\n",
       " 'if you ar razr own you must hav thi',\n",
       " 'needless say wast my money',\n",
       " 'what wast money and tim']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lancaster = [stem_with_lancaster(x.split()) for x in sents_processed]\n",
    "\n",
    "lancaster = [\" \".join(i) for i in lancaster]\n",
    "\n",
    "lancaster[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some weird changes occur, e.g. very->veri, quality->qualiti, value->valu, â€¦**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD/IDF\n",
    "\n",
    "Term Document Inverse Document Frequency (TD/IDF). This is a measure of the relative \n",
    "importance of a word within a document, in the context of multiple documents . In our \n",
    "case here, multiple reviews.\n",
    "\n",
    "We start with the TD part â€” this is simply a normalized frequency of the word in the \n",
    "document:\n",
    "\n",
    "(word count in document) / (total words in document)\n",
    "The IDF is a weighting of the uniquess of the word across all of the documents. Here is \n",
    "the complete formula of TD/IDF:\n",
    "\n",
    "                     td_idf(t,d) = wc(t,d)/wc(d) / dc(t)/dc()\n",
    "\n",
    "where:\n",
    "\n",
    "* wc(t,d) = # of occurrences of term t in doc d\n",
    "\n",
    "* wc(d) = # of words in doc d\n",
    "\n",
    "* dc(t) = # of docs that contain at least 1 occurrence of term t\n",
    "\n",
    "* dc() = # of docs in collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, letâ€™s create a bag of words and normalise the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", preprocessor = None, stop_words =  'english', max_features = 6000, ngram_range=(1,5))\n",
    "\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "data_features_tfidf = tfidf_transformer.fit_transform(data_features)\n",
    "data_mat = data_features_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now data_mat is our Document-Term matrix. Input is ready to put into model. Letâ€™s create \n",
    "Training and Test sets. Here, I split the data into a training set of 2500 sentences and \n",
    "a test set of 500 sentences (of which 250 are positive and 250 negative). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_index = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_index = list(set(range(len(labels))) - set(test_index))\n",
    "train_data = data_mat[train_index,]\n",
    "train_labels = y[train_index]\n",
    "test_data = data_mat[test_index,]\n",
    "test_labels = y[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------\n",
    "# ### TextBlob\n",
    "\n",
    "# TextBlob : Linguistic researchers have labeled the sentiment of words based on their \n",
    "# domain expertise. Sentiment of words can vary based on where it is in a sentence. The \n",
    "# TextBlob module allows us to take advantage of these labels. TextBlod finds all the words \n",
    "# and phrases that it can assign polarity and subjectivity to, and average all of them \n",
    "# together.\n",
    "\n",
    "# Sentiment Labels : Each word in a corpus is labeled in terms of polarity and subjectivity \n",
    "# (there are more labels as well, but weâ€™re going to ignore them for now). A corpusâ€™ \n",
    "# sentiment is the average of these.\n",
    "\n",
    "# Polarity : How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
    "# Subjectivity : How subjective, or opinionated a word is. 0 is fact. +1 is very much \n",
    "# an opinion.\n",
    "\n",
    "\n",
    "# ### Create polarity function and subjectivity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "pol_list = [pol(x) for x in sents_processed]\n",
    "sub_list = [sub(x) for x in sents_processed]\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-------------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
